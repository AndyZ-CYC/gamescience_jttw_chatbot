# 西游记智能问答系统

本项目是一个基于《西游记》原著文本的智能问答系统，采用了RAG（Retrieval-Augmented Generation）架构，结合了语义检索和关键词检索，通过主流大语言模型生成回答，并提供了简洁的Web用户界面。

## 项目架构

此项目主要由四个模块组成：

1. **文本数据清洗与数据库构建**：处理原始文本数据，分段存储，并建立向量数据库
2. **检索模块**：包含语义检索和关键词检索两种方法，以及它们的组合策略
3. **回答生成模块**：使用AI模型基于检索内容生成回答
4. **前端界面**：提供Web交互界面

## 模块详细说明

### 1. 文本数据清洗与数据库构建

#### 模块说明

此模块负责离线部分的文本数据清洗与向量数据库的构建，为后续语义检索做好基础。

#### 相关文件

- **RAG数据库构建.ipynb**：包含数据处理和数据库构建的全流程代码
- **data/all_paragraphs.json**：处理后的文本段落存储文件
- **data/西游记_UTF-8.txt**：粗略清洗（移除页码与字数统计）后经过UTF-8编码的原始文本
- **data/西游记.txt**：原始文本文件

#### 主要功能

- 文本清洗：去除不必要的标点和格式
- 文本分段：按照章节和段落进行合理分割
- 向量生成：使用OpenAI的Embedding模型(text-embedding-3-large)生成文本向量
- Pinecone数据库：建立和管理向量索引与云端向量数据库

#### 问题与后续开发方向

- 可尝试不同文本段落分隔方式（如改为自然段分割，或更改每个文本段落字数限制）
- 可尝试使用其他Embedding模型
- 此模块代码可以被部分复用，后续如果有构建更完善RAG数据库的需求（如包含多本文献），可进行适当更改与应用

### 2. 检索模块

#### 模块说明

此模块负责使用不同检索方法检索与用户问题相关的文本片段与相关段落/章节信息，以后续Prompt文本构建。

#### 相关文件和目录

- **retriever/**：检索模块的主目录
  - **__init__.py**：模块初始化和函数导出
  - **semantic_retriever.py**：语义检索实现
  - **keyword_retriever.py**：关键词检索实现
  - **combined_retriever.py**：组合检索策略实现
- **语义与关键词检索.ipynb**：检索方法的开发和测试笔记本

#### 主要功能

- **语义检索（Semantic Retriever）**：
  - 近义扩展：通过GPT-4o根据用户输入内容生成含义近似的表达方式，以增强语义检索的召回值(recall)
  - 多查询检索：将扩展后的多个查询结果合并
  
- **关键词检索（Keyword Retriever）**：
  - 关键词提取：使用GPT-4o从用户问题中提取关键词
  - 同义词扩展：使用GPT-4o对关键词进行同义词扩展
  - 模糊匹配（fuzz）：使用模糊匹配算法匹配文本段落
  
- **组合检索**：
  - 参数选择：根据用户问题所含关键词判断是否需要列举全部情况，半自动调整top_k参数
  - 结果融合：智能融合语义和关键词检索结果
  - 排序优化：根据相关性重新排序检索结果，并返回前top_k个结果

#### 问题与后续开发方向

- 关键词检索在测试问题中的准确率通常略低于语义检索，并且目前关键词检索在组合检索中的权重也略低，但考虑到对某些特定问题会有奇效，后续可以进行权重调整和关键词分数加权的开发
- 目前检索使用的是RAG的基本逻辑，而Rerank模型在某些情况表现会优于普通的RAG，但考虑到《西游记》单本小说的样本量较低、Rerank模型时间成本偏大等问题，暂时没有采用
- RAG在匹配用户的某些抽象问题（比如"列举所有一打多的场景"）时效果不佳，因此如果想提升这类问题的精度可能需要更完善的数据库或者更精细的标签

### 3. 回答生成模块

#### 功能说明

此模块为项目核心模块，基于检索内容生成连贯、准确的回答。开发架构支持多种不同大语言模型供用户选择，并提供一定背景、要求与预设来增强回答效果。

#### 相关文件和目录

- **generator/**：回答生成模块的主目录
  - **__init__.py**：模块初始化和函数导出
  - **answer_generator.py**：答案生成器实现
- **回答生成.ipynb**：生成方法的开发和测试笔记本

#### 主要功能

- 初始化配置：设置API密钥和模型参数
- Prompt构建：根据检索结果构建有效的Prompt文本
- 回答生成：调用大语言模型生成回答

#### 问题与后续开发方向

- 目前代码只支持openai，deepseek，和anthropic三个开发商的模型，可以通过修改部分代码模块添加更多可支持的模型

### 4. 前端UI模块

#### 模块说明

此模块负责提供简介的的Web界面，支持模型选择和问题输入，并实时显示回答和状态

#### 相关文件和目录

- **frontend/**：前端模块的主目录
  - **app.py**：Flask应用主文件
  - **run.py**：启动脚本
  - **README.md**：前端使用说明
  - **static/**：静态资源目录
    - **css/style.css**：样式表
    - **js/app.js**：前端JavaScript代码
  - **templates/**：
    - **index.html**：HTML模板

#### 主要功能

- 用户界面：简洁直观的聊天界面
- 多模型支持：可选择不同的生成模型
- 响应式设计：适配不同设备屏幕
- Markdown渲染：支持格式化文本输出
- 状态指示：显示系统处理状态

#### 后续开发方向

- 添加更多功能，比如支持用户自行输入API密钥
- 美化界面，添加外部图片素材等

## 运行指南

### 方法1：本地运行

##### 准备环境

1. 安装Python 3.8+
2. 安装依赖包：
   ```
   pip install openai pinecone-client flask markdown python-dotenv
   ```
3. 准备API密钥

##### 启动系统

1. 启动前端服务：
   ```
   python frontend/run.py
   ```
2. 访问Web界面：
   ```
   http://127.0.0.1:5000
   ```

### 方法2：使用在线Web版本

系统已部署在线上，可以通过以下方式访问：

#### 访问方式
- **在线地址**：[西游记问答系统](https://xiyouji-qa.onrender.com)
- **支持设备**：电脑、平板和手机等各种设备（自适应界面）

#### 使用说明
1. 打开网页后系统会自动初始化（初次加载可能需要等待约20-30秒）
2. 在输入框中输入有关《西游记》的问题
3. 从下拉菜单中选择使用的模型（默认为GPT-4o）
4. 点击发送按钮或按回车键提交问题
5. 系统会显示正在思考状态，稍后展示回答

#### 特点
- 无需安装，直接通过浏览器访问
- 同时支持多个用户独立使用
- 所有API密钥和配置已预先设置，用户无需提供自己的密钥

## 使用工具汇总

- **向量数据库**：Pinecone
- **嵌入模型**：OpenAI - text-embedding-3-large
- **生成模型**：OpenAI的GPT-4o，GPT-4.5-preview模型，Deepseek的V3，R1模型
- **后端框架**：Flask
- **前端技术**：HTML, CSS, JavaScript, Bootstrap

## 其他注意事项与问题

- 目前GPT-4.5-preview模型较为昂贵，虽然可能是表现最好的模型，但可能需要谨慎使用
- Deepseek-R1模型由于深度思索的特性所以响应时间较长
- 由于地区服务限制，暂时没有引进Anthropic的API密钥
- 目前本模型能够准确的引用西游记原文内容，并基本杜绝AI的幻觉问题（hallucination）。然而在进行列举工作的时候，仍有可能出现列举不完全的问题；与此同时，模型对一些抽象的问题回答准确性会有一定的降低。

### 模型性能与超时说明

- **推荐模型**：GPT-4o是系统默认模型，综合性能与速度最佳，推荐首选。
- **特殊需求模型**：
  - **GPT-4.5-preview**：对于更复杂的问题可能有更好表现，但响应时间较长，可能偶尔超时。
  - **DeepSeek系列模型**：响应时间不稳定，在网络波动大的情况下可能超时。
- **超时处理**：系统内置了自动容错机制，当非GPT-4o模型超时时，会自动回退到GPT-4o模型并给出提示。
- **大量内容检索**：提问时包含"列举"、"全部"、"八十一难"等关键词的问题会触发高召回率模式，这类问题建议使用GPT-4o模型以避免超时。

## 最新优化说明

### 2025-04-16 系统稳定性优化

1. **移除线程处理机制**：
   - 取消了Flask请求处理中的线程等待模式，改为直接API调用
   - 解决了Gunicorn工作进程超时与线程等待超时的冲突问题
   - 提高了系统在处理长时间运行模型请求时的稳定性

2. **移除API调用超时限制**：
   - 取消了OpenAI和DeepSeek API调用中的显式timeout参数
   - 依赖底层HTTP库的默认超时机制，避免过早中断模型响应
   - 使GPT-4.5-preview和DeepSeek系列模型能够有足够时间思考和生成回答

3. **增强用户体验**：
   - 前端添加了处理时间计数器，在模型响应过程中显示已等待时间
   - 长时间请求（超过60秒）自动提供用户友好提示
   - 超时处理机制更完善，提供更明确的错误信息

4. **服务器配置优化**：
   - 添加了专用Gunicorn配置文件(gunicorn.conf.py)
   - 增加工作进程超时时间至300秒（5分钟）
   - 优化工作进程类型和连接设置

这些优化使系统能更好地处理各种模型的响应特性，特别是对响应时间较长的GPT-4.5-preview和DeepSeek系列模型。

© 2025 张耀元 | 西游记问答系统 
